{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68a44b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ æ‰¾åˆ°å¯¹åº”çš„JSONæ–‡ä»¶: /Users/liangzijun/Documents/2025é‡‘ç§å­/knowledege_graph/ai-knowledge-graph/é“è·¯æ–½å·¥è®¸å¯æµç¨‹å›¾è°±.json\n",
      "âœ… æˆåŠŸä»JSONæ–‡ä»¶åŠ è½½äº† 88 ä¸ªä¸‰å…ƒç»„\n",
      "âœ… çŸ¥è¯†å›¾è°±æŠ¥å‘Šå·²ä¿å­˜ä¸º: /Users/liangzijun/Documents/2025é‡‘ç§å­/knowledege_graph/ai-knowledge-graph/é“è·¯æ–½å·¥è®¸å¯æµç¨‹å›¾è°±.md\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import html2text\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime\n",
    "\n",
    "def html_to_markdown(html_content):\n",
    "    \"\"\"\n",
    "    å°† HTML å†…å®¹è½¬æ¢ä¸º Markdown æ ¼å¼\n",
    "    \"\"\"\n",
    "    converter = html2text.HTML2Text()\n",
    "    converter.ignore_links = False       # ä¿ç•™é“¾æ¥\n",
    "    converter.ignore_images = False      # ä¿ç•™å›¾ç‰‡\n",
    "    converter.body_width = 0             # ä¸è‡ªåŠ¨æ¢è¡Œ\n",
    "    converter.single_line_break = True   # å•æ¢è¡Œç¬¦æ¢è¡Œ\n",
    "\n",
    "    return converter.handle(html_content)\n",
    "\n",
    "def extract_triples_from_json(json_file):\n",
    "    \"\"\"ä»JSONæ–‡ä»¶åŠ è½½ä¸‰å…ƒç»„æ•°æ®\"\"\"\n",
    "    try:\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ åŠ è½½JSONæ–‡ä»¶å¤±è´¥: {e}\")\n",
    "        return None\n",
    "\n",
    "def analyze_graph_structure(triples):\n",
    "    \"\"\"åˆ†æå›¾è°±ç»“æ„å¹¶ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯\"\"\"\n",
    "    if not triples:\n",
    "        return {}\n",
    "    \n",
    "    # åŸºæœ¬ç»Ÿè®¡\n",
    "    total_triples = len(triples)\n",
    "    entities = set()\n",
    "    predicates = []\n",
    "    inferred_count = 0\n",
    "    \n",
    "    for triple in triples:\n",
    "        entities.add(triple.get('subject', ''))\n",
    "        entities.add(triple.get('object', ''))\n",
    "        predicates.append(triple.get('predicate', ''))\n",
    "        if triple.get('inferred', False):\n",
    "            inferred_count += 1\n",
    "    \n",
    "    # è°“è¯ç»Ÿè®¡\n",
    "    predicate_counts = Counter(predicates)\n",
    "    \n",
    "    # å®ä½“è¿æ¥åº¦ç»Ÿè®¡\n",
    "    entity_connections = defaultdict(int)\n",
    "    for triple in triples:\n",
    "        entity_connections[triple.get('subject', '')] += 1\n",
    "        entity_connections[triple.get('object', '')] += 1\n",
    "    \n",
    "    return {\n",
    "        'total_triples': total_triples,\n",
    "        'total_entities': len(entities),\n",
    "        'total_predicates': len(set(predicates)),\n",
    "        'inferred_triples': inferred_count,\n",
    "        'original_triples': total_triples - inferred_count,\n",
    "        'predicate_counts': predicate_counts,\n",
    "        'entity_connections': entity_connections,\n",
    "        'top_entities': sorted(entity_connections.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    }\n",
    "\n",
    "def generate_knowledge_graph_report(triples, title=\"çŸ¥è¯†å›¾è°±åˆ†ææŠ¥å‘Š\"):\n",
    "    \"\"\"ç”ŸæˆçŸ¥è¯†å›¾è°±ä¸“é—¨çš„MarkdownæŠ¥å‘Š\"\"\"\n",
    "    if not triples:\n",
    "        return \"âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ä¸‰å…ƒç»„æ•°æ®\"\n",
    "    \n",
    "    # åˆ†æå›¾è°±ç»“æ„\n",
    "    stats = analyze_graph_structure(triples)\n",
    "    \n",
    "    # ç”ŸæˆMarkdownå†…å®¹\n",
    "    markdown_content = []\n",
    "    \n",
    "    # æ ‡é¢˜å’ŒåŸºæœ¬ä¿¡æ¯\n",
    "    markdown_content.append(f\"# {title}\")\n",
    "    markdown_content.append(f\"\\nğŸ“… **ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    markdown_content.append(f\"\\n---\\n\")\n",
    "    \n",
    "    # å›¾è°±ç»Ÿè®¡\n",
    "    markdown_content.append(\"## ğŸ“Š å›¾è°±ç»Ÿè®¡\")\n",
    "    markdown_content.append(f\"- **æ€»å®ä½“æ•°**: {stats['total_entities']}\")\n",
    "    markdown_content.append(f\"- **æ€»å…³ç³»æ•°**: {stats['total_triples']}\")\n",
    "    markdown_content.append(f\"- **åŸå§‹å…³ç³»**: {stats['original_triples']}\")\n",
    "    markdown_content.append(f\"- **æ¨ç†å…³ç³»**: {stats['inferred_triples']}\")\n",
    "    markdown_content.append(f\"- **è°“è¯ç±»å‹**: {stats['total_predicates']}\")\n",
    "    \n",
    "    # é‡è¦å®ä½“\n",
    "    markdown_content.append(f\"\\n## ğŸ”— é‡è¦å®ä½“ (æŒ‰è¿æ¥åº¦æ’åº)\")\n",
    "    markdown_content.append(\"| å®ä½“ | è¿æ¥æ•° |\")\n",
    "    markdown_content.append(\"|------|--------|\")\n",
    "    for entity, count in stats['top_entities']:\n",
    "        markdown_content.append(f\"| {entity} | {count} |\")\n",
    "    \n",
    "    # å¸¸è§å…³ç³»ç±»å‹\n",
    "    markdown_content.append(f\"\\n## ğŸ”„ å¸¸è§å…³ç³»ç±»å‹\")\n",
    "    markdown_content.append(\"| å…³ç³»ç±»å‹ | å‡ºç°æ¬¡æ•° |\")\n",
    "    markdown_content.append(\"|----------|----------|\")\n",
    "    for predicate, count in stats['predicate_counts'].most_common(10):\n",
    "        markdown_content.append(f\"| {predicate} | {count} |\")\n",
    "    \n",
    "    # æŒ‰ç±»å‹åˆ†ç»„æ˜¾ç¤ºä¸‰å…ƒç»„\n",
    "    original_triples = [t for t in triples if not t.get('inferred', False)]\n",
    "    inferred_triples = [t for t in triples if t.get('inferred', False)]\n",
    "    \n",
    "    if original_triples:\n",
    "        markdown_content.append(f\"\\n## ğŸ“‹ åŸå§‹å…³ç³» ({len(original_triples)} ä¸ª)\")\n",
    "        markdown_content.append(\"| ä¸»è¯­ | è°“è¯­ | å®¾è¯­ |\")\n",
    "        markdown_content.append(\"|------|------|------|\")\n",
    "        for triple in sorted(original_triples, key=lambda x: x.get('subject', ''))[:20]:  # åªæ˜¾ç¤ºå‰20ä¸ª\n",
    "            subject = triple.get('subject', '').replace('|', '\\\\|')\n",
    "            predicate = triple.get('predicate', '').replace('|', '\\\\|')\n",
    "            obj = triple.get('object', '').replace('|', '\\\\|')\n",
    "            markdown_content.append(f\"| {subject} | {predicate} | {obj} |\")\n",
    "        \n",
    "        if len(original_triples) > 20:\n",
    "            markdown_content.append(f\"\\n*... è¿˜æœ‰ {len(original_triples) - 20} ä¸ªåŸå§‹å…³ç³»*\")\n",
    "    \n",
    "    if inferred_triples:\n",
    "        markdown_content.append(f\"\\n## ğŸ§  æ¨ç†å…³ç³» ({len(inferred_triples)} ä¸ª)\")\n",
    "        markdown_content.append(\"| ä¸»è¯­ | è°“è¯­ | å®¾è¯­ |\")\n",
    "        markdown_content.append(\"|------|------|------|\")\n",
    "        for triple in sorted(inferred_triples, key=lambda x: x.get('subject', ''))[:15]:  # åªæ˜¾ç¤ºå‰15ä¸ª\n",
    "            subject = triple.get('subject', '').replace('|', '\\\\|')\n",
    "            predicate = triple.get('predicate', '').replace('|', '\\\\|')\n",
    "            obj = triple.get('object', '').replace('|', '\\\\|')\n",
    "            markdown_content.append(f\"| {subject} | {predicate} | {obj} |\")\n",
    "        \n",
    "        if len(inferred_triples) > 15:\n",
    "            markdown_content.append(f\"\\n*... è¿˜æœ‰ {len(inferred_triples) - 15} ä¸ªæ¨ç†å…³ç³»*\")\n",
    "    \n",
    "    # ç”Ÿæˆå®ä½“ç½‘ç»œå›¾ï¼ˆç®€åŒ–ç‰ˆï¼‰\n",
    "    markdown_content.append(f\"\\n## ğŸŒ æ ¸å¿ƒå®ä½“å…³ç³»ç½‘ç»œ\")\n",
    "    markdown_content.append(\"```\")\n",
    "    markdown_content.append(\"ä¸»è¦å®ä½“åŠå…¶ç›´æ¥å…³ç³»:\")\n",
    "    \n",
    "    # é€‰æ‹©å‰5ä¸ªæœ€é‡è¦çš„å®ä½“\n",
    "    for entity, _ in stats['top_entities'][:5]:\n",
    "        markdown_content.append(f\"\\nğŸ”¸ {entity}:\")\n",
    "        related_triples = [t for t in triples if t.get('subject') == entity or t.get('object') == entity]\n",
    "        for triple in related_triples[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªå…³ç³»\n",
    "            if triple.get('subject') == entity:\n",
    "                markdown_content.append(f\"  â†’ {triple.get('predicate')} â†’ {triple.get('object')}\")\n",
    "            else:\n",
    "                markdown_content.append(f\"  â† {triple.get('predicate')} â† {triple.get('subject')}\")\n",
    "    \n",
    "    markdown_content.append(\"```\")\n",
    "    \n",
    "    # ç”ŸæˆMermaidå›¾\n",
    "    markdown_content.append(f\"\\n## ğŸ¯ å…³ç³»å›¾è°± (Mermaid)\")\n",
    "    markdown_content.append(\"```mermaid\")\n",
    "    markdown_content.append(\"graph TD\")\n",
    "    \n",
    "    # é€‰æ‹©é‡è¦çš„å…³ç³»ç”ŸæˆMermaidå›¾\n",
    "    node_map = {}\n",
    "    node_counter = 0\n",
    "    \n",
    "    # ä¸ºé‡è¦å®ä½“åˆ›å»ºèŠ‚ç‚¹\n",
    "    for entity, _ in stats['top_entities'][:8]:  # åªæ˜¾ç¤ºå‰8ä¸ªé‡è¦å®ä½“\n",
    "        node_id = f\"N{node_counter}\"\n",
    "        node_map[entity] = node_id\n",
    "        # æ¸…ç†å®ä½“åç§°ä¸­çš„ç‰¹æ®Šå­—ç¬¦\n",
    "        clean_entity = entity.replace('\"', '').replace(\"'\", \"\")\n",
    "        markdown_content.append(f'    {node_id}[\"{clean_entity}\"]')\n",
    "        node_counter += 1\n",
    "    \n",
    "    # æ·»åŠ å…³ç³»\n",
    "    added_edges = set()\n",
    "    for triple in triples:\n",
    "        subject = triple.get('subject', '')\n",
    "        obj = triple.get('object', '')\n",
    "        predicate = triple.get('predicate', '')\n",
    "        \n",
    "        if subject in node_map and obj in node_map:\n",
    "            edge_key = (subject, obj, predicate)\n",
    "            if edge_key not in added_edges:\n",
    "                subject_node = node_map[subject]\n",
    "                obj_node = node_map[obj]\n",
    "                # æ¸…ç†è°“è¯åç§°\n",
    "                clean_predicate = predicate.replace('\"', '').replace(\"'\", \"\")\n",
    "                markdown_content.append(f'    {subject_node} -->|\"{clean_predicate}\"| {obj_node}')\n",
    "                added_edges.add(edge_key)\n",
    "    \n",
    "    markdown_content.append(\"```\")\n",
    "    \n",
    "    return \"\\n\".join(markdown_content)\n",
    "\n",
    "def convert_html_file_to_knowledge_graph_markdown(input_html_path, output_md_path=None):\n",
    "    \"\"\"\n",
    "    è¯»å–çŸ¥è¯†å›¾è°±HTMLæ–‡ä»¶å¹¶è½¬æ¢ä¸ºä¸“é—¨çš„çŸ¥è¯†å›¾è°±MarkdownæŠ¥å‘Š\n",
    "    \"\"\"\n",
    "    if not os.path.exists(input_html_path):\n",
    "        print(f\"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {input_html_path}\")\n",
    "        return\n",
    "\n",
    "    if output_md_path is None:\n",
    "        base_name = os.path.splitext(input_html_path)[0]\n",
    "        output_md_path = f\"{base_name}_çŸ¥è¯†å›¾è°±æŠ¥å‘Š.md\"\n",
    "\n",
    "    # é¦–å…ˆå°è¯•ä»å¯¹åº”çš„JSONæ–‡ä»¶åŠ è½½ç»“æ„åŒ–æ•°æ®\n",
    "    json_file = input_html_path.replace('.html', '.json')\n",
    "    triples = None\n",
    "    \n",
    "    if os.path.exists(json_file):\n",
    "        print(f\"ğŸ“„ æ‰¾åˆ°å¯¹åº”çš„JSONæ–‡ä»¶: {json_file}\")\n",
    "        triples = extract_triples_from_json(json_file)\n",
    "        \n",
    "        if triples:\n",
    "            print(f\"âœ… æˆåŠŸä»JSONæ–‡ä»¶åŠ è½½äº† {len(triples)} ä¸ªä¸‰å…ƒç»„\")\n",
    "            # ä»æ–‡ä»¶åæå–æ ‡é¢˜\n",
    "            title = os.path.basename(input_html_path).replace('.html', '') + \" - çŸ¥è¯†å›¾è°±åˆ†ææŠ¥å‘Š\"\n",
    "            \n",
    "            # ç”Ÿæˆä¸“é—¨çš„çŸ¥è¯†å›¾è°±æŠ¥å‘Š\n",
    "            markdown_content = generate_knowledge_graph_report(triples, title)\n",
    "            \n",
    "            # ä¿å­˜æŠ¥å‘Š\n",
    "            with open(output_md_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(markdown_content)\n",
    "            \n",
    "            print(f\"âœ… çŸ¥è¯†å›¾è°±æŠ¥å‘Šå·²ä¿å­˜ä¸º: {output_md_path}\")\n",
    "            return\n",
    "    \n",
    "    # å¦‚æœæ²¡æœ‰JSONæ–‡ä»¶ï¼Œåˆ™ä½¿ç”¨ä¼ ç»Ÿçš„HTMLè½¬æ¢æ–¹æ³•\n",
    "    print(\"âš ï¸ æœªæ‰¾åˆ°å¯¹åº”çš„JSONæ–‡ä»¶ï¼Œä½¿ç”¨HTMLç›´æ¥è½¬æ¢...\")\n",
    "    \n",
    "    with open(input_html_path, 'r', encoding='utf-8') as f:\n",
    "        html_content = f.read()\n",
    "\n",
    "    markdown_content = html_to_markdown(html_content)\n",
    "\n",
    "    with open(output_md_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(markdown_content)\n",
    "\n",
    "    print(f\"âœ… å·²æˆåŠŸè½¬æ¢å¹¶ä¿å­˜ä¸º: {output_md_path}\")\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "input_html = \"/Users/liangzijun/Documents/2025é‡‘ç§å­/knowledege_graph/ai-knowledge-graph/é“è·¯æ–½å·¥è®¸å¯æµç¨‹å›¾è°±.html\"\n",
    "output_md = \"/Users/liangzijun/Documents/2025é‡‘ç§å­/knowledege_graph/ai-knowledge-graph/é“è·¯æ–½å·¥è®¸å¯æµç¨‹å›¾è°±.md\"\n",
    "\n",
    "convert_html_file_to_knowledge_graph_markdown(input_html, output_md)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
